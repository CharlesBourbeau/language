{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Shakespeare Dataset for Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to read dataset locally.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "datapath = './data/input.txt'\n",
    "\n",
    "if not os.path.exists(os.path.dirname(datapath)):\n",
    "    os.mkdir(os.path.dirname(datapath))\n",
    "\n",
    "try:\n",
    "    print('Trying to read dataset locally.')\n",
    "    with open(datapath) as f:\n",
    "        data = f.read()\n",
    "    print('Done.')\n",
    "except FileNotFoundError:\n",
    "    print('Not Found. Downloading...')\n",
    "    response = requests.get('https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt')\n",
    "    if response.status_code == 200:\n",
    "        print('Download successfull.')\n",
    "        data = response.text\n",
    "        with open(datapath, 'w') as f:\n",
    "            f.write(data)\n",
    "        print('Saved dataset locally.')\n",
    "    else:\n",
    "        raise RuntimeError(f'Failed to fetch data from {DATASET_URL}: status code: {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text data contains\n",
      "\t1,115,394 symbols\n",
      "\tWith a vocabulary of size: 65 \n",
      "\t\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "n_symbols = len(data)\n",
    "vocab = list(sorted(set(data)))\n",
    "print(f'Text data contains' \\\n",
    "        f'\\n\\t{n_symbols:,} symbols' \\\n",
    "        f'\\n\\tWith a vocabulary of size: {len(vocab)}',\n",
    "        '\\n\\t' + ''.join(vocab).encode('unicode_escape').decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "For now, we stick to a very simple tokenization procedure, which encode each individual character to a numerical representation.\n",
    "\n",
    "This means that the number of tokens is predetermined and equal to the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTokenizer:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.vocab = list(sorted(set(data)))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        self.ctoi = {ch:i for i, ch in enumerate(vocab)}\n",
    "        self.itoc = {i:ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "    def encode(self, s):\n",
    "        return [self.ctoi[c] for c in s]\n",
    "    \n",
    "    def decode(self, l):\n",
    "        return ''.join(self.itoc[i] for i in l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test string: Hello there! $.o\n",
      "Encoded: [20, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43, 2, 1, 3, 8, 53]\n",
      "Decoded: Hello there! $.o\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CharacterTokenizer(data)\n",
    "\n",
    "test_string = 'Hello there! $.o'\n",
    "print('Test string:', test_string)\n",
    "print('Encoded:', tokenizer.encode(test_string))\n",
    "print('Decoded:', tokenizer.decode(tokenizer.encode(test_string)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now encode our entire dataset such that it can be ingested by a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394])\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0])\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(data)\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "print(tokens.shape)\n",
    "print(tokens[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tokens: 1,003,854\n",
      "Val tokens: 111,540\n"
     ]
    }
   ],
   "source": [
    "split = int(0.9 * len(tokens))\n",
    "train_data = tokens[:split]\n",
    "val_data = tokens[split:]\n",
    "\n",
    "print(f'Train tokens: {train_data.shape[0]:,}')\n",
    "print(f'Val tokens: {val_data.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Length\n",
    "\n",
    "Data needs to be subdivided into chunks of a specific size to allow sampling different random parts of text, from which the model will learn to predict autoregressively each token inside a chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given context (chunk), we can emit a prediction for each of the token contained inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [18, 47, 56, 57, 58, 1, 15, 47]\n",
      "y: [47, 56, 57, 58, 1, 15, 47, 58]\n",
      "t0: From context: [18] -> Predict 47\n",
      "t1: From context: [18, 47] -> Predict 56\n",
      "t2: From context: [18, 47, 56] -> Predict 57\n",
      "t3: From context: [18, 47, 56, 57] -> Predict 58\n",
      "t4: From context: [18, 47, 56, 57, 58] -> Predict 1\n",
      "t5: From context: [18, 47, 56, 57, 58, 1] -> Predict 15\n",
      "t6: From context: [18, 47, 56, 57, 58, 1, 15] -> Predict 47\n",
      "t7: From context: [18, 47, 56, 57, 58, 1, 15, 47] -> Predict 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:context_length]\n",
    "y = train_data[1:context_length+1]\n",
    "print('x:', x.tolist())\n",
    "print('y:', y.tolist())\n",
    "for i in range(context_length):\n",
    "    context = x[:i+1]\n",
    "    print(f't{i}: From context: {context.tolist()} -> Predict {y[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
